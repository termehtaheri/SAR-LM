# ğŸ§ SAR-LM: Symbolic Audio Reasoning with Large Language Models

**Authors:** Termeh Taheri, Yinghao Ma, and Emmanouil Benetos  
**Affiliation:** Centre for Digital Music (C4DM), Queen Mary University of London  
**Paper:** _SAR-LM: Symbolic Audio Reasoning with Large Language Models_ (to appear on arXiv, 2025)

---

## Overview

**SAR-LM** is a modular framework for **symbolic audio reasoning** â€” combining feature extraction, captioning, and reasoning within a single, transparent pipeline.

Instead of treating audio as raw waveforms only, SAR-LM converts it into **symbolic representations** (speech transcripts, event tags, note sequences, chord progressions, etc.) and feeds them into large language models such as **Gemini**, **Qwen-3**, and **Qwen-Omni** for reasoning over sound.

This design enables interpretability, reproducibility, and controlled evaluation on reasoning benchmarks such as **MMAU**, **MMAR**, and **OmniBench**.

---

## âœ¨ Key Features

- ğŸ”Š **Unified Extractors** â€“ PANNs, Whisper, MT3, Musicnn, Chordino, and DAWN emotion features.  
- ğŸ§  **Multi-Backend Reasoning** â€“ Gemini 2.5 Pro, Qwen-3, and Qwen-Omni backends for symbolic QA.  
- ğŸ—£ï¸ **Captioning Pipelines** â€“ Symbolic, Mixed, and End-to-End audio caption generation.  
- ğŸ§© **Fully Modular Design** â€“ Each extractor and reasoner is containerized and can run independently.  
- ğŸ“Š **Reproducible Outputs** â€“ JSON-based I/O for easy integration with benchmarks and analysis tools.

---

## ğŸ§± Repository Structure

```
SAR-LM/
â”‚
â”œâ”€â”€ src/sar_lm/
â”‚   â”œâ”€â”€ extractors/         # Individual feature extractors (PANNs, Whisper, etc.)
â”‚   â”œâ”€â”€ captions/           # Symbolic, mixed, and end-to-end captioners
â”‚   â”œâ”€â”€ reasoners/          # Gemini, Qwen3, Qwen-Omni reasoning backends
â”‚   â”œâ”€â”€ prompts/            # All prompt templates (centralized)
â”‚   â””â”€â”€ pipelines/          # Orchestrators for extraction, merging, captioning, reasoning
â”‚
â”œâ”€â”€ examples/               # Sample audios and QA examples
â”œâ”€â”€ outputs/                # Example outputs (features, captions, reasoning results)
â”œâ”€â”€ docker/                 # Dockerfiles for all modules
â”œâ”€â”€ requirements/           # Environment-specific dependencies
â”œâ”€â”€ Makefile                # Workflow shortcuts
â”œâ”€â”€ CITATION.cff            # Citation metadata
â”œâ”€â”€ pyproject.toml          # Package and dependency configuration
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

### Option 1: Local (Recommended for testing)
```bash
git clone https://github.com/termehtaheri/SAR-LM.git
cd SAR-LM
python3 -m venv venv
source venv/bin/activate
pip install -r requirements/base.txt
```

### Option 2: Docker
Each extractor and reasoning module has its own `Dockerfile` under `docker/`.  
You can build them individually:
```bash
docker-compose build panns
docker-compose build whisper
```

or run all at once:
```bash
docker-compose up -d
```
(All services will start in idle mode and can be triggered independently.)

---

## ğŸš€ Usage

### 1. Extract features
```bash
PYTHONPATH=src python -m sar_lm.pipelines.extract_pipeline \
  --audio_dir examples \
  --output_dir outputs/features_panns \
  --device cpu
```

### 2. Merge features
```bash
PYTHONPATH=src python -m sar_lm.pipelines.merge_features \
  --panns outputs/features_panns/panns_features.json \
  --whisper outputs/features_whisper/whisper_features.json \
  --mt3 outputs/features_mt3/mt3_features.json \
  --emotion outputs/features_dawn/dawn_emotion_features.json \
  --musicnn outputs/features_musicnn/musicnn_features.json \
  --chordino outputs/features_chordino/chordino_features.json \
  --output outputs/features_merged/features_merged.json
```

### 3. Generate captions
Symbolic captioning:
```bash
PYTHONPATH=src python -m sar_lm.pipelines.captioning_pipeline \
  --mode symbolic \
  --audio_dir examples \
  --features outputs/features_merged/features_merged.json \
  --output outputs/captions/symbolic_captions.json
```

### 4. Run reasoning
```bash
PYTHONPATH=src python -m sar_lm.pipelines.reasoning_pipeline \
  --reasoner qwen3 \
  --features outputs/features_merged/features_merged.json \
  --qa examples/sample_qa.json \
  --output outputs/reasoning/qwen3_results.json
```

---

## ğŸ” API Keys

If you use **Gemini** models for captioning or reasoning, set your API key in a `.env` file:

```
GEMINI_API_KEY=your_api_key_here
```

---

## ğŸ§© Reproducibility

All environments are defined in:
- `requirements/*.txt` â€“ for lightweight installs  
- `docker/extractors/` â€“ containerized extractors  
- `requirements/mt3_env.yml` â€“ specialized MT3 setup  

To build everything cleanly:
```bash
make env
```

---

## ğŸ“š Citation

If you use SAR-LM in your work, please cite:

```
@article{taheri2025sarlm,
  title={SAR-LM: Symbolic Audio Reasoning with Large Language Models},
  author={Taheri, Termeh and Ma, Yinghao and Benetos, Emmanouil},
  journal={arXiv preprint arXiv:TBD},
  year={2025}
}
```

---

## ğŸ§  Acknowledgements

This project was developed at the **Centre for Digital Music (C4DM)**,  
**Queen Mary University of London**, as part of Termeh Taheriâ€™s MSc research project supervised by Prof. Emmanouil Benetos.  

Special thanks to Yinghao Ma for guidance on benchmarking and integration.

---

## ğŸªª License

This repository is released under the **MIT License**.  
See [LICENSE](LICENSE) for details.
